{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lAsyeOwFnoX"
   },
   "source": [
    "# CSCE 623 Homework Assignment 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46d-KBtJJxN1"
   },
   "source": [
    "### Student Name:  <font color=\"red\">LASTNAME, FIRSTNAME</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Yna07rL4Jz43"
   },
   "source": [
    "### Date: <font color=\"red\">April XX, 2023</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification in 2-feature datasets\n",
    "\n",
    "Instructions:\n",
    "* Review all provided code before starting your work - this instructor has provided hints and tips throughout the code\n",
    "* This assignment is composed of 2 parts\n",
    "    * Load, split, and explore the data\n",
    "    * Fit models and evaluate performance\n",
    "* Complete the 12 numbered STEPS which contain (STUDENT CODE REQUIRED) and (STUDENT MARKDOWN RESPONSE REQUIRED) activities\n",
    "* Remember to restart the kernel and rerun all cells before submitting the assignment\n",
    "* Submit only the Jupyter Notebook (.ipynb) file - do not submit the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as col\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL STUDENT CODE\n",
    "\n",
    "If you need any additional imports not listed above, import them below so that the instructor is aware of what additional imports you are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------- START STUDENT CODE HERE -------------------\n",
    "\n",
    "# student package import statements here if needed\n",
    "\n",
    "\n",
    "#------------- END STUDENT CODE HERE -------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Load, explore, and discuss the datasets (steps 1-7)\n",
    "\n",
    "Each dataset is stored in a Comma Separated Value (CSV) file.  The first row is a header row.  After the first row, each row is an observation, the first 2 columns are features (\"X1\" and \"X2\" and the third column is a classification category (encoded as 0 or 1)\n",
    "\n",
    "In this part, you will perform several actions repeatedly on each of 3 datasets to load, split, and explore the data.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 (DATASET 1):  Load Dataset 1 (STUDENT CODE)\n",
    "\n",
    "Ensure that the data sets (e.g., `dataset1.csv`) is in the same directory as your code.  Then accomplish the next two actions in the code cell below\n",
    "\n",
    "Load the data using `pandas` function `read_csv`.   \n",
    "* Use the `header` parameter to indicate row 0 as a header row\n",
    "* Use the `names` parameter to name the 2 features `X1` and `X2` and the third column as `Class`\n",
    "* Use the `index_col` parameter to tell the function that column 0 is the index\n",
    "\n",
    "Use `sklearn` to partition the data into test and training sets (`df1_train` and `df1_test`):\n",
    "* The sets must be of exactly equal size \n",
    "* The train and test sets must each have the *exact proportion of class distributions* which were present in the full set before the split (if the percentage was Class0 = A and Class1=B, then those same percentages must be in the training set and in the test set).\n",
    "\n",
    "Hint1:  Loading and splitting each require only one line of code.   Instead of making up your own way of achieving the desired result, it is recommened to review the APIs and examples on the internet.\n",
    "\n",
    "Hint2:  If you are using loops, breaking things apart then joining them back together, counting class memberships or doing other things besides a single call to the appropriate package, then you are probably being inefficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1\n",
    "\n",
    "df1 = None #placeholder to load the dataframe in\n",
    "df1_train = None #placeholder for training set partition\n",
    "df1_test = None #placeholder for test set partition\n",
    "randstate = 42\n",
    "\n",
    "#------------- START STUDENT CODE HERE -------------------\n",
    "\n",
    "\n",
    "#------------- END STUDENT CODE HERE -------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Create a reusable function for data exploration (STUDENT CODE)\n",
    "\n",
    "Create a `data_explore` function to \n",
    "\n",
    "* display statistics about the data (using pandas `describe()` on *each* feature)\n",
    "* describe the covariance of *each* Class (0 and 1) using `.cov`\n",
    "\n",
    "* Make a Pariplot with class-selected colors\n",
    "    * use Seaborn (`sns`) to create a `pairplot` of the dataset\n",
    "    * make sure to use the `pairplot` function call attribute `hue` to make the color of the histogram created from a set of observations be associated with its class    \n",
    "\n",
    "* Make a set of histograms using pandas\n",
    "    * for each class (0 and 1) create a pandas histogram (`hist`) of each feature (using the default hist colors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#STEP 2\n",
    "\n",
    "def data_explore(df):\n",
    "    \n",
    "    #------------- START STUDENT CODE HERE -------------------\n",
    "\n",
    "    \n",
    "    #------------- END STUDENT CODE HERE -------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the training set from the first dataset (instructor code) \n",
    "\n",
    "Using only the training data, this code calls the function created above - the `data_explore` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_explore(df1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 (DATASET 1): Discussion (STUDENT MARKDOWN RESPONSE REQUIRED)\n",
    "\n",
    "In the markdown cell below, describe the data attributes you noticed during exploration of the training set 1.   \n",
    "\n",
    "In particular, focus on the attributes which may facilitate classification or make classification of the data more challenging.  Hint:  look for differences in the statistics between classes.  Note that some differences might be for a specific features while other differences might involve both features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>STUDENT ANSWER BELOW</font>   \n",
    "\n",
    "<font color='green'> words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4 (DATASET 2):  Load, Split and Explore Dataset 2 (STUDENT CODE)\n",
    "\n",
    "Follow the steps on Dataset 2 as you did for Dataset 1.  Remember to only explore the training set (not the test set)\n",
    "* Load the CSV for `dataset2.csv`\n",
    "* Split the test and train data (stratified by class, with equal portions of the data going to each set)\n",
    "* Call `data_explore` to explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4\n",
    "\n",
    "#------------- START STUDENT CODE HERE -------------------\n",
    "\n",
    "\n",
    "\n",
    "#------------- END STUDENT CODE HERE -------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5 (DATASET 2): Discussion (STUDENT MARKDOWN CELL)\n",
    "\n",
    "In the markdown cell below, describe the data attributes you noticed during exploration of the training set 2.   \n",
    "\n",
    "In particular, focus on the attributes which may facilitate classification or make classification of the data more challenging.  Hint:  look for differences in the statistics between classes.  Note that some differences might be for a specific features while other differences might involve both features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>STUDENT ANSWER BELOW</font>   \n",
    "\n",
    "<font color='green'>words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6 (DATASET 3): Load, Split and Explore Dataset 3  (STUDENT CODE)\n",
    "\n",
    "Follow the steps on Dataset 2 as you did for Dataset 1.  Remember to only explore the training set (not the test set)\n",
    "* Load the CSV for `dataset3.csv`\n",
    "* Split the test and train data (stratified by class, with equal portions of the data going to each set)\n",
    "* Call `data_explore` to explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 6\n",
    "\n",
    "#------------- START STUDENT CODE HERE -------------------\n",
    "\n",
    "\n",
    "\n",
    "#------------- END STUDENT CODE HERE -------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7 (DATASET 3): Discussion (STUDENT MARKDOWN CELL)\n",
    "\n",
    "In the markdown cell below, describe the data attributes you noticed during exploration of the training set.   \n",
    "\n",
    "In particular, focus on the attributes which may facilitate classification or make classification of the data more challenging.  Hint:  look for differences in the statistics between classes.  Note that some differences might be for a specific features while other differences might involve both features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "<font color='green'>STUDENT ANSWER BELOW</font>   \n",
    "\n",
    "<font color='green'>words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - Fitting Classification Models & Reporting Performance on the Test Set  (steps 8-12)\n",
    "\n",
    "\n",
    "In this part, you will perform step 8 to obtain classification performance (accuracy) for each model on each of the datasets.\n",
    "\n",
    "Then in steps 9-11 you will focus on dataset 3 to produce confusion matrices and ROC curves on just dataset 3\n",
    "\n",
    "Finally, in step 12 you will draw some conclusions about the performance of the models and make recommendations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Classifiers (INSTRUCTOR-PROVIDED CODE)\n",
    "\n",
    "In this code, we use a function to train 3 models (Logistic Regression, Linear Discriminant Analysis, and Quadratic Discriminant Analysis) on each of the three datasets (9 models total).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifiers(data):\n",
    "    X = data.loc[:,['X1', 'X2']]\n",
    "    y = data.Class\n",
    "    log_reg = LogisticRegression(solver='lbfgs')\n",
    "    log_reg.fit(X, y)\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X, y)\n",
    "    \n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    qda.fit(X, y)\n",
    "    \n",
    "    return {'Logistic_Regression':log_reg, 'LDA':lda, 'QDA':qda}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models1 = build_classifiers(df1_train)  #contains one model for each type of classifier on dataset 1\n",
    "models2 = build_classifiers(df2_train)\n",
    "models3 = build_classifiers(df3_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are returned in a python dictionary can be accessed using the (key, model) accessors like these examples:\n",
    "* `models1['Logistic_Regression']` returns the Logistic Regression model that was fit on dataset 1\n",
    "* `models3['LDA']` returns the LDA model that was fit on dataset 3\n",
    "* `models2['QDA']` returns the QDA model that was fit on dataset 2\n",
    "\n",
    "Recall that \n",
    "* the Logistic Regression model is `sklearn.linear_model.LogisticRegression.fit()`\n",
    "* the LDA model was fit using `sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit()`\n",
    "* the QDA model was fit using `sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "For each dataset, for each model, determine the class prediction probabilities using `.predict_proba()` on the features from the dataset provided to the function (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8:  Determining classifier Accuracy (STUDENT CODE REQUIRED)\n",
    "\n",
    "For each of the models above, for each test dataset, report Accuracy on the test set:\n",
    "\n",
    "Report this in a table format by storing the values in the columns and rows of a new dataframe and then displaying the dataframe\n",
    "* use a separate row for each dataset\n",
    "* use a separate column for each model's performance\n",
    "\n",
    "Hint:  use `sklearn.metrics.accuracy_score` for this activity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 8\n",
    "\n",
    "accdf = None #placeholder for the dataframe which will contain performance scores\n",
    "\n",
    "#------------- START STUDENT CODE HERE -------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------- END STUDENT CODE HERE -------------------\n",
    "\n",
    "accdf.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9:  Confusion Matrix (STUDENT CODE REQUIRED)\n",
    "\n",
    "In Dataset 3, there was a meaningful difference in performance between the models.  \n",
    "\n",
    "For each model (logistic regression, LDA, QDA), create a confusion matrix of its performance on test dataset 3\n",
    "* Tip:  use `sklearn` `ConfusionMatrixDisplay`\n",
    "* The True label should identify the rows of the matrix while the predicted label identifies the columns\n",
    "* Ensure each matrix is properly titled with the name of the model used to create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 9\n",
    "\n",
    "#------------- START STUDENT CODE HERE -------------------\n",
    "\n",
    "\n",
    "\n",
    "#------------- END STUDENT CODE HERE -------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function to use a classifier to estimate class probabilities on a dataset (INSTRUCTOR PROVIDED CODE)\n",
    "\n",
    "In later code cells this function will be called with each dataset and the 3 models used to fit models to that dataset. It relies on the `sklearn` model's `.predicte_proba` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(models, X):\n",
    "    \"\"\" Returns a dictionary of predicted proability vectors using models stored in the input dictionary 'models' on the feature data 'X'\n",
    "    params:  \n",
    "    models - a dictionary of fitted classification models with key equal to the name of the model\n",
    "    X - the values of a dataset obtained\"\"\"\n",
    "    predicts = {}\n",
    "    \n",
    "    for key, model in models.items():\n",
    "        predicts[key] = model.predict_proba(X)\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## STEP 10: Call the `predict_probs` function to estimate class probabilities on each test set (STUDENT CODE REQUIRED)\n",
    "\n",
    "Using the instructor-provided `predict_probs(models, X)` function, obtain the dictionary of predicitons of each classifier on the results of each of the 3 test datasets.  \n",
    "\n",
    "Store the returned dictionary of predictions in `predicts1`, `predicts2`, and `predicts3` (1 dictionary per dataset)\n",
    "\n",
    "Hint:  to obtain the parameter `X` for the input value to the function, use the `pandas` dataframe accessor `datasetname.loc[:,['X1','X2']]` where `datasetname` is the name of a test set dataframe like `df1_test`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 10\n",
    "\n",
    "predicts1 = {} #placeholder for predictions of each of the three models on test dataset 1\n",
    "predicts2 = {} #placeholder for predictions of each of the three models on test dataset 2\n",
    "predicts3 = {} #placeholder for predictions of each of the three models on test dataset 3\n",
    "\n",
    "#------------- START STUDENT CODE HERE -------------------\n",
    "\n",
    "\n",
    "#------------- END STUDENT CODE HERE -------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instructor code  - spot check model predictions\n",
    "print(\"logistic regression\\n\", predicts1[\"Logistic_Regression\"][0:5] )\n",
    "print(\"LDA\\n\",predicts3[\"LDA\"][0:5] )\n",
    "print(\"QDA\\n\",predicts2[\"QDA\"][0:5] )\n",
    "\n",
    "#'Logistic_Regression':log_reg, 'LDA':lda, 'QDA':qda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 11:  Create ROC Curves for each test dataset (STUDENT CODE REQUIRED)\n",
    "\n",
    "For each of the three test datasets, display a single ROC curve with each of the three model performances on it.  You should show one graph per dataset, and on each graph, include the ROCs for each of the three models on the graph, using colored lines with legends.\n",
    "\n",
    "Each graph should have:\n",
    "* A title which includes the name of the dataset\n",
    "* The horizontal axis label should contain the words \"False Positive Rate\"\n",
    "* The vertical axis label should contain the words \"True Positive Rate\"\n",
    "* One ROC line per model, with the legend appropriately named\n",
    "\n",
    "Assume that Class \"1\" is the postive class for this activity\n",
    "\n",
    "Hint:  See `sklearn.metrics` `RocCurveDisplay.from_predictions` which will provide the curves and the AUC for each ROC line.  To use this, you will need to provide \n",
    "* The correct class for each observation, as contained in the test dataset 'Class' column. \n",
    "* The probabilities for the positive class obtained from the `predict_probs` of the model of interest.  This is located in column index = 1  (the right column)\n",
    "* The `pos_label` Class identifier - which is 1 since we are assuming 1 is the positive class\n",
    "* The `name` for the ROC line which should be a descriptive name of the classfier model used to produce the line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 11\n",
    "\n",
    "# model names:  'Logistic_Regression':log_reg, 'LDA':lda, 'QDA':qda\n",
    "\n",
    "for test_predicts,test_dataset, df_names in zip([predicts1, predicts2, predicts3],[df1_test,df2_test,df3_test],['Test Dataset 1','Test Dataset 2', 'Test Dataset 3']):\n",
    "    \n",
    "    #------------- START STUDENT CODE HERE -------------------\n",
    "\n",
    "    \n",
    "    \n",
    "    #------------- END STUDENT CODE HERE -------------------\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 12 - Conclusions from the ROC Curves (STUDENT MARKDOWN RESPONSE REQUIRED)\n",
    "\n",
    "Review the ROC Curves resulting from step 11 and answer the following quesions:\n",
    "\n",
    "Q1:  For which datasets (1, 2, and 3) does it matter which model we choose to make predictions, and what evidence supports this?\n",
    "Q2:  For the dataset(s) which matter(s) (from the set {1,2,3}), which models are the best to choose, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='green'>STUDENT ANSWER Q1 BELOW</font>   \n",
    "\n",
    "<font color='green'>Q1: words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font color='green'>STUDENT ANSWER Q2 BELOW</font>   \n",
    "\n",
    "\n",
    "<font color='green'>Q2: words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
